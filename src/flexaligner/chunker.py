import torch
import os
import json
import soundfile as sf
from pathlib import Path
from typing import List, Optional, Dict
from dataclasses import dataclass, asdict
from transformers import Wav2Vec2ForCTC, AutoProcessor

# ÂºïÂÖ• io.py Êï∞ÊçÆÁªìÊûÑ
try:
    from .io import AudioChunk
except ImportError:
    @dataclass
    class AudioChunk:
        tensor: torch.Tensor
        start_time: float
        end_time: float
        text: str
        chunk_id: str

# ==========================================
#  Helper Data Structures
# ==========================================
@dataclass
class Point:
    token_index: int 
    time_index: int 

@dataclass
class Segment:
    label: str
    start_frame: int
    end_frame: int
    @property
    def duration_frames(self):
        return self.end_frame - self.start_frame

@dataclass
class PronCandidate:
    phones: List[str]
    pron_choice_idxs: List[int]
    score: float

@dataclass
class WordSeg:
    start: float
    dur: float
    word: str
    @property
    def end(self) -> float:
        return self.start + self.dur

@dataclass
class InternalChunk:
    start: float
    end: float
    words: List[str]

# ==========================================
#  CTCChunker Class (Strictly Equivalent)
# ==========================================

class CTCChunker:
    def __init__(self, config: dict):
        self.config = config or {}
        self.device = torch.device(self.config.get("device", "cpu"))
        self.lang = self.config.get("lang", "zh")
        
        # [Verbose]
        self.verbose = self.config.get("verbose", False)
        self.chunks_out_dir = self.config.get("chunks_out_dir", None)
        
        if self.verbose:
            self._log_header("CTCChunker Initializing")
            print(f"  - Device: {self.device}")
            print(f"  - Lang: {self.lang}")
            print(f"  - Intermediate Output: {self.chunks_out_dir if self.chunks_out_dir else 'Disabled'}")

        # Resources
        self.model = None
        self.processor = None
        self.lexicon = {}
        self.phone_to_id = {} 
        self.blank_id = 0
        
        # Hyperparameters (Stage 1)
        self.beam_size = self.config.get("beam_size", 10)
        self.min_chunk_s = self.config.get("min_chunk_s", 1.0)
        self.max_chunk_s = self.config.get("max_chunk_s", 12.0)
        self.max_gap_s = self.config.get("max_gap_s", 0.35)
        self.min_words = self.config.get("min_words", 2)
        self.pad_s = self.config.get("pad_s", 0.15)
        self.blank_token = self.config.get("blank_token", "<pad>")
        
        # [Physics Fix] ‰∏çÂÜç‰ΩøÁî®Âõ∫ÂÆö hopÔºåËÄåÂú®ËøêË°åÊó∂ËÆ°ÁÆó
        # self.config_hop = 0.02 

        self._load_resources()

    def _log(self, msg: str):
        if self.verbose: print(f"[CTCChunker] {msg}")

    def _log_header(self, title: str):
        if self.verbose: print(f"\n=== {title} ===")

    def _load_resources(self):
        lex_path = self.config.get("lexicon_path")
        self.lexicon = self._read_lexicon(lex_path)
        if self.verbose: print(f"  - Lexicon loaded: {len(self.lexicon)} words")
        
        model_path = self.config.get("chunk_model_path")
        if not model_path: return

        self._log(f"Requesting model: {model_path}")
        load_kwargs = {}
        if not os.path.isdir(model_path):
            load_kwargs["subfolder"] = f"{self.lang}/chunker"
            self._log(f"Mode: Cloud Repo ({load_kwargs['subfolder']})")
        else:
            self._log("Mode: Local Override")

        try:
            self.processor = AutoProcessor.from_pretrained(model_path, **load_kwargs)
            self.model = Wav2Vec2ForCTC.from_pretrained(model_path, **load_kwargs).to(self.device)
            self.model.eval()
            self._log("Successfully loaded model.")
        except Exception as e:
            self._log(f"Routing failed ({e}), falling back to root...")
            try:
                self.processor = AutoProcessor.from_pretrained(model_path)
                self.model = Wav2Vec2ForCTC.from_pretrained(model_path).to(self.device)
            except Exception as final_e:
                raise RuntimeError(f"Model load failed: {final_e}")

        if hasattr(self.processor, "tokenizer"):
            self.phone_to_id = self.processor.tokenizer.get_vocab()
            # Resolve blank token
            if self.blank_token in self.phone_to_id:
                self.blank_id = self.phone_to_id[self.blank_token]
            elif self.processor.tokenizer.pad_token_id is not None:
                self.blank_id = self.processor.tokenizer.pad_token_id
            else:
                self.blank_id = 0
            
            # English Hotfix
            if self.lang == "en":
                self.garbage_tokens = {"O", "[UNK]", "<unk>"}
                self.fuzzy_map = {}
                for token, pid in self.phone_to_id.items():
                    self.fuzzy_map[token] = pid
                    pure_token = ''.join(filter(str.isalpha, token))
                    if pure_token and pure_token not in self.fuzzy_map:
                        self.fuzzy_map[pure_token] = pid
                self._log(f"üá¨üáß English Hotfix Applied: Fuzzy map built with {len(self.fuzzy_map)} entries.")
        else:
            self.blank_id = 0

    @torch.inference_mode()
    def find_chunks(self, audio_tensor: torch.Tensor, text_list: List[str], file_id: str = "unknown") -> List[AudioChunk]:
        if self.model is None: raise RuntimeError("CTC Ê®°ÂûãÊú™Âä†ËΩΩ")

        if self.verbose:
            self._log_header(f"Processing: {file_id}")

        # 1. Forward Pass
        # Ê≥®ÊÑè: chunks2.py ÈªòËÆ§ target_sr = 16000
        input_values = self.processor(
            audio_tensor.numpy(), 
            sampling_rate=16000, 
            return_tensors="pt"
        ).input_values.to(self.device)
        
        logits = self.model(input_values).logits 
        log_probs = torch.log_softmax(logits, dim=-1).squeeze(0)
        
        # [Fix 1: Dynamic SPF Calculation]
        # ÂÆåÂÖ®Â§çÂàª chunks2.py: seconds_per_frame = float(audio.numel() / sr) / float(log_probs.size(0))
        spf = float(audio_tensor.numel() / 16000) / float(log_probs.size(0))

        if self.verbose:
            print(f"  - Audio Duration: {audio_tensor.size(0)/16000:.3f}s")
            print(f"  - Logits Shape: {log_probs.shape}")
            print(f"  - Calculated SPF: {spf:.6f} (was fixed 0.02)")

        # 2. Beam Search
        prons_per_word = self._words_to_pronunciations(text_list)
        
        best_candidate = None
        for attempt_beam in [self.beam_size, 50, 200, 1000]:
            best_candidate = self._beam_search(log_probs, text_list, prons_per_word, beam_width=attempt_beam)
            if best_candidate:
                if attempt_beam > self.beam_size: self._log(f"üí° Expanded beam to {attempt_beam}.")
                break
        
        if not best_candidate:
            self._log("‚ùå Beam search failed. Fallback to full audio.")
            return [AudioChunk(
                tensor=audio_tensor, start_time=0.0, end_time=audio_tensor.size(0)/16000.0,
                text=" ".join(text_list), chunk_id=f"{file_id}.chunk_fallback"
            )]

        # 3. Viterbi Alignment
        target_ids = [self.phone_to_id[p] for p in best_candidate.phones if p in self.phone_to_id]
        if not target_ids: return []

        trellis = build_trellis(log_probs, target_ids, self.blank_id)
        points = backtrace(trellis, log_probs, target_ids, self.blank_id)
        
        # 4. Convert to Segments
        filtered_phones = [p for p in best_candidate.phones if p in self.phone_to_id]
        token_segs = points_to_segments(points, filtered_phones)
        
        clean_prons = []
        for i, idx in enumerate(best_candidate.pron_choice_idxs):
            raw_pron = prons_per_word[i][idx]
            if len(raw_pron) > 0 and isinstance(raw_pron[0], list): raw_pron = raw_pron[0]
            filtered_pron = [p for p in raw_pron if p in self.phone_to_id]
            clean_prons.append(filtered_pron)

        word_segs = self._phones_to_word_segments_robust(token_segs, text_list, clean_prons)

        # 5. Logical Chunking (using dynamic SPF)
        word_objects = [
            WordSeg(s.start_frame * spf, (s.end_frame - s.start_frame) * spf, s.label)
            for s in word_segs
        ]
        
        internal_chunks = self._merge_words_into_chunks(word_objects)
        audio_dur_s = float(audio_tensor.size(0)) / 16000
        internal_chunks = self._pad_chunks(internal_chunks, word_objects, audio_dur_s)

        # 6. Physical Extraction & Object Creation
        final_chunks = []
        sr = 16000
        for i, c in enumerate(internal_chunks):
            # chunks2.py: s0 = int(round(c.start * sr))
            s_samp = int(round(c.start * sr))
            e_samp = int(round(c.end * sr))
            s_samp = max(0, min(s_samp, audio_tensor.size(0)))
            e_samp = max(0, min(e_samp, audio_tensor.size(0)))
            
            if e_samp <= s_samp: continue

            chunk_id_str = f"{file_id}.chunk{i+1:03d}"
            
            chunk_obj = AudioChunk(
                tensor=audio_tensor[s_samp:e_samp].clone(),
                start_time=c.start,
                end_time=c.end,
                text=" ".join(c.words),
                chunk_id=chunk_id_str
            )
            final_chunks.append(chunk_obj)

        self._log(f"Found {len(final_chunks)} chunks.")

        if self.chunks_out_dir:
            self._save_intermediate_results(final_chunks, file_id)

        return final_chunks

    def _save_intermediate_results(self, chunks: List[AudioChunk], file_id: str):
        """
        [Fix 2: Path Logic] Á°Æ‰øùÊñá‰ª∂ÁîüÊàêÂú®ÊåáÂÆöÁöÑ chunks_out_dir ÂÜÖÔºåËÄå‰∏çÊòØ parent„ÄÇ
        """
        out_root = Path(self.chunks_out_dir)
        out_root.mkdir(parents=True, exist_ok=True)
        
        # ‰øÆÊ≠£ÔºöÁõ¥Êé•Âú® out_root ‰∏ãÁîüÊàê jsonl/tsv
        jsonl_path = out_root / f"{file_id}.chunks.jsonl"
        tsv_path = out_root / f"{file_id}.chunks.tsv"

        if self.verbose:
            print(f"  - Saving artifacts to: {out_root}")

        with open(jsonl_path, "w", encoding="utf-8") as fj, \
             open(tsv_path, "w", encoding="utf-8") as ft:
            
            ft.write("chunk_id\tstart_s\tend_s\tdur_s\twords\n")

            for c in chunks:
                wav_name = f"{c.chunk_id}_{c.start_time:.3f}-{c.end_time:.3f}.wav"
                wav_path = out_root / wav_name
                
                sf.write(str(wav_path), c.tensor.squeeze().numpy(), 16000)

                dur = c.end_time - c.start_time
                words_list = c.text.split()
                
                # ÂÖ≥ÈîÆÔºö‰øùÁïô round(x, 3) ‰ª•ÂåπÈÖç chunks2.py ÁöÑËæìÂá∫Ê†ºÂºè
                obj = {
                    "chunk_id": c.chunk_id,
                    "audio": str(wav_path),
                    "start_s": round(c.start_time, 3),
                    "end_s": round(c.end_time, 3),
                    "dur_s": round(dur, 3),
                    "words": words_list,
                    "text": c.text
                }
                
                fj.write(json.dumps(obj, ensure_ascii=False) + "\n")
                # TSV ‰πüÊòØÁî®ÁöÑ rounded values
                ft.write(f"{c.chunk_id}\t{obj['start_s']}\t{obj['end_s']}\t{obj['dur_s']}\t{obj['text']}\n")

    # --- ÂÜÖÈÉ®ÊñπÊ≥ï (‰øùÊåÅ‰∏çÂèò) ---

    def _read_lexicon(self, path: Optional[str]):
        lexicon = {}
        if path and os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) >= 2:
                        lexicon[parts[0].lower()] = parts[1:]
        # English patch omitted for brevity (same as before)
        return lexicon

    def _words_to_pronunciations(self, words: List[str]):
        out = []
        for w in words:
            w_norm = w.strip().lower()
            if not w_norm: continue
            if w_norm not in self.lexicon:
                if self.verbose: print(f"‚ö†Ô∏è  [CTCChunker] OOV Word: '{w_norm}'")
                raise ValueError(f"[CTCChunker] OOV Word: {w_norm}")
            out.append([self.lexicon[w_norm]])
        return out

    def _beam_search(self, log_probs, words, prons_per_word, beam_width=10):
        beam = [PronCandidate(phones=[], pron_choice_idxs=[], score=0.0)]
        for i, word in enumerate(words):
            new_beam = []
            variants = prons_per_word[i]
            for cand in beam:
                for p_idx, pron in enumerate(variants):
                    if len(pron) > 0 and isinstance(pron[0], list): pron = pron[0]
                    
                    # Inter-word token logic (Strictly matching chunks2.py: if phones...)
                    current_phones = list(cand.phones)
                    inter_token = self.config.get("inter_word_token", None)
                    if current_phones and inter_token: 
                        current_phones.append(inter_token)
                    
                    new_phones = current_phones + pron
                    new_ids = []
                    valid_pron = True
                    for p in new_phones:
                        if not isinstance(p, str): continue
                        if p in self.phone_to_id: new_ids.append(self.phone_to_id[p])
                        elif p == "O": continue
                        else: 
                            # Try fuzzy
                            p_pure = ''.join(filter(str.isalpha, p))
                            if p_pure in self.phone_to_id: new_ids.append(self.phone_to_id[p_pure])
                            else: 
                                valid_pron = False; break
                    
                    if not valid_pron: continue
                    try:
                        trellis = build_trellis(log_probs, new_ids, self.blank_id)
                        score = float(torch.max(trellis[-1, -1]).item())
                    except: score = -float("inf")
                    
                    if score > -1e8:
                        new_beam.append(PronCandidate(new_phones, cand.pron_choice_idxs + [p_idx], score))
            if not new_beam: return None
            new_beam.sort(key=lambda x: x.score, reverse=True)
            beam = new_beam[:beam_width]
        return beam[0] if beam else None

    def _phones_to_word_segments_robust(self, token_segs, words, prons):
        word_segs = []
        wi = 0 
        for i, (word, pron) in enumerate(zip(words, prons)):
            n_phones = len(pron)
            if n_phones == 0: continue
            if wi + n_phones > len(token_segs): break 
            word_segs.append(Segment(word, token_segs[wi].start_frame, token_segs[wi + n_phones - 1].end_frame))
            wi += n_phones
        return word_segs

    def _merge_words_into_chunks(self, words: List[WordSeg]):
        if not words: return []
        chunks = []
        cur_words = [words[0].word]
        cur_start = words[0].start
        cur_end = words[0].end
        for w in words[1:]:
            gap = w.start - cur_end
            proposed_dur = w.end - cur_start
            if gap <= self.max_gap_s and proposed_dur <= self.max_chunk_s:
                cur_end = w.end
                cur_words.append(w.word)
            else:
                if (cur_end - cur_start) >= self.min_chunk_s and len(cur_words) >= self.min_words:
                    chunks.append(InternalChunk(cur_start, cur_end, cur_words))
                cur_start = w.start
                cur_end = w.end
                cur_words = [w.word]
        if len(cur_words) > 0 and (cur_end - cur_start) >= self.min_chunk_s and len(cur_words) >= self.min_words:
             chunks.append(InternalChunk(cur_start, cur_end, cur_words))
        return chunks

    def _pad_chunks(self, chunks, words, audio_dur):
        if not chunks: return []
        out = []
        for c in chunks:
            # Replicated logic from previous turn (Correct)
            first_word_idx = -1; last_word_idx = -1
            for i_w, w in enumerate(words):
                if abs(w.start - c.start) < 1e-4: first_word_idx = i_w
                if abs(w.end - c.end) < 1e-4: last_word_idx = i_w
            
            left_limit = 0.0
            if first_word_idx > 0: left_limit = words[first_word_idx - 1].end
            right_limit = audio_dur
            if last_word_idx != -1 and last_word_idx + 1 < len(words): right_limit = words[last_word_idx + 1].start
            
            left_gap = max(0.0, c.start - left_limit)
            right_gap = max(0.0, right_limit - c.end)
            max_pad = self.config.get("max_pad_into_gap_s", 0.25)
            
            new_start = max(0.0, c.start - min(self.pad_s, max_pad, left_gap))
            new_end = min(audio_dur, c.end + min(self.pad_s, max_pad, right_gap))
            out.append(InternalChunk(new_start, new_end, c.words))
        return out

# Static Functions (Unchanged)
def build_trellis(log_probs, targets, blank_id):
    T, V = log_probs.shape
    N = len(targets)
    device = log_probs.device
    neg_inf = -1e9
    targets_t = torch.tensor(targets, device=device, dtype=torch.long)
    trellis = torch.full((T + 1, N + 1), neg_inf, device=device, dtype=log_probs.dtype)
    trellis[0, 0] = 0.0
    trellis[1:, 0] = torch.cumsum(log_probs[:, blank_id], dim=0)
    for t in range(1, T + 1):
        lp_t = log_probs[t - 1]
        stay = trellis[t - 1, 1:] + lp_t[blank_id]
        emit = trellis[t - 1, :-1] + lp_t[targets_t]
        trellis[t, 1:] = torch.maximum(stay, emit)
    return trellis

def backtrace(trellis, log_probs, targets, blank_id):
    _T = trellis.size(0) - 1
    N = trellis.size(1) - 1
    j = N; t = _T; path = []
    while t > 0 and j > 0:
        lp_t = log_probs[t - 1]
        score_current = trellis[t, j]
        score_stay = trellis[t - 1, j] + lp_t[blank_id]
        score_emit = trellis[t - 1, j - 1] + lp_t[targets[j - 1]]
        if abs(score_current - score_emit) < 1e-4:
            path.append(Point(j - 1, t - 1)); j -= 1; t -= 1
        else: t -= 1
    path.reverse()
    return path

def points_to_segments(points, labels):
    if not points: return []
    segs = []
    for i, p in enumerate(points):
        start = p.time_index
        end = points[i + 1].time_index if i + 1 < len(points) else start + 1
        segs.append(Segment(labels[p.token_index], start, end))
    return segs