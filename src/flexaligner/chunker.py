import torch
import json
from pathlib import Path
from typing import List, Dict
from dataclasses import dataclass
from transformers import Wav2Vec2ForCTC, AutoProcessor
import os
# å¼•å…¥æˆ‘ä»¬åœ¨ io.py å®šä¹‰çš„æ•°æ®ç»“æ„
from .io import AudioChunk

# ==========================================
#  Helper Data Structures (å†…éƒ¨ä½¿ç”¨)
# ==========================================

@dataclass
class Point:
    token_index: int  # index in target token sequence
    time_index: int   # frame index

@dataclass
class Segment:
    label: str
    start_frame: int
    end_frame: int

    @property
    def duration_frames(self):
        return self.end_frame - self.start_frame

@dataclass
class PronCandidate:
    phones: List[str]
    pron_choice_idxs: List[int]
    score: float

@dataclass
class WordSeg:
    start: float
    dur: float
    word: str
    @property
    def end(self) -> float:
        return self.start + self.dur

@dataclass
class InternalChunk:
    """ä¸­é—´çŠ¶æ€çš„ Chunkï¼Œåªå­˜æ—¶é—´ä¿¡æ¯ï¼Œè¿˜æ²¡åˆ‡ Tensor"""
    start: float
    end: float
    words: List[str]

# ==========================================
#  CTCChunker Class (æ ¸å¿ƒé€»è¾‘)
# ==========================================

class CTCChunker:
    def __init__(self, config: dict):
        self.config = config or {}
        # ç»Ÿä¸€ä»é…ç½®è¯»å–è®¾å¤‡
        self.device = torch.device(self.config.get("device", "cpu"))
        
        # èµ„æºå ä½ç¬¦
        self.model = None
        self.processor = None
        self.lexicon = {}
        self.phone_to_id = {}
        self.blank_id = 0
        
        # åŠ è½½è¶…å‚
        self.beam_size = self.config.get("beam_size", 10)
        self.min_chunk_s = self.config.get("min_chunk_s", 1.0)
        self.max_chunk_s = self.config.get("max_chunk_s", 12.0)
        self.max_gap_s = self.config.get("max_gap_s", 0.35)
        self.min_words = self.config.get("min_words", 2)
        self.pad_s = self.config.get("pad_s", 0.15)
        
        # å¯åŠ¨åŠ è½½
        self._load_resources()


    def _load_resources(self):
        """åŠ è½½è¯å…¸å’Œæ¨¡å‹é…ç½®ï¼ˆé€šç”¨åŒ–é‡æ„ç‰ˆï¼‰"""
        # 1. åŠ è½½ Lexicon
        lex_path = self.config.get("lexicon_path")
        if lex_path:
            self.lexicon = self._read_lexicon(Path(lex_path))
        
        # 2. åŠ è½½ Phones JSON
        phone_path = self.config.get("phone_json_path")
        if phone_path:
            with open(phone_path, "r", encoding="utf-8") as f:
                self.phone_to_id = json.load(f)
        
        # 3. åŠ è½½æ¨¡å‹ (Global Loading Logic)
        model_path = self.config.get("chunk_model_path")
        if not model_path: return

        print(f"[CTCChunker] Requesting model: {model_path}")

        # --- é…ç½®åŠ è½½å‚æ•° ---
        load_kwargs = {}
        
        # ç­–ç•¥ A: å®˜æ–¹ç‰¹ä¾›ç‰ˆ (åŒ…å«å­ç›®å½•)
        # åªæœ‰æˆ‘ä»¬çš„å®˜æ–¹åº“éœ€è¦è¿™ä¸ª subfolder å‚æ•°ï¼Œå…¶ä»–é€šç”¨æ¨¡å‹ä¸éœ€è¦
        if "USTCPhonetics/FlexAligner" in model_path:
            print("[CTCChunker] Mode: Official Repo (with subfolder)")
            load_kwargs["subfolder"] = "hf_phs"
            
        # ç­–ç•¥ B: ç”¨æˆ·æŒ‡å®šçš„æœ¬åœ°è·¯å¾„
        elif os.path.isdir(model_path):
            print("[CTCChunker] Mode: Local Override")
            # æœ¬åœ°è·¯å¾„ä¸éœ€è¦é¢å¤–å‚æ•°ï¼Œtransformers ä¼šè‡ªåŠ¨è¯†åˆ«
            
        # ç­–ç•¥ C: é€šç”¨ Hugging Face æ¨¡å‹ (å¦‚ English æ¨¡å‹)
        # e.g., "facebook/wav2vec2-base-960h"
        else:
            print("[CTCChunker] Mode: Generic HF Hub (Cache -> Download)")
            # ä¸éœ€è¦ subfolderï¼Œç›´æ¥åŠ è½½

        # --- ç»Ÿä¸€æ‰§è¡ŒåŠ è½½ ---
        # transformers çš„ from_pretrained é»˜è®¤é€»è¾‘å°±æ˜¯ï¼š
        # 1. æ£€æŸ¥ model_path æ˜¯å¦ä¸ºæœ¬åœ°æ–‡ä»¶å¤¹ -> æ˜¯åˆ™åŠ è½½
        # 2. æ£€æŸ¥ ~/.cache/huggingface ä¸‹æ˜¯å¦æœ‰ç¼“å­˜ -> æ˜¯åˆ™åŠ è½½
        # 3. è”ç½‘ä¸‹è½½ -> ä¸‹è½½å¹¶ç¼“å­˜
        try:
            from transformers import AutoProcessor, Wav2Vec2ForCTC
            
            self.processor = AutoProcessor.from_pretrained(model_path, **load_kwargs)
            self.model = Wav2Vec2ForCTC.from_pretrained(model_path, **load_kwargs).to(self.device)
            self.model.eval()
            print(f"[CTCChunker] Successfully loaded model from {model_path}")
            
        except Exception as e:
            raise RuntimeError(
                f"Model load failed.\n"
                f"Path: {model_path}\n"
                f"Args: {load_kwargs}\n"
                f"Error: {e}\n"
                "Check network connection or model name."
            )

        # 4. ç¡®å®š blank_id (ä¿æŒä¸å˜)
        blank_token = self.config.get("blank_token", "<pad>")
        if blank_token in self.phone_to_id:
            self.blank_id = self.phone_to_id[blank_token]
        elif hasattr(self.processor, "tokenizer") and self.processor.tokenizer.pad_token_id is not None:
            self.blank_id = self.processor.tokenizer.pad_token_id
        else:
            self.blank_id = 0
    @torch.inference_mode()
    def find_chunks(self, audio_tensor: torch.Tensor, text_list: List[str]) -> List[AudioChunk]:
        """
        [ä¸»å…¥å£] æ‰§è¡Œ Stage 1 å®Œæ•´æµç¨‹ï¼š
        Audio -> LogProbs -> BeamSearch -> Viterbi -> WordSegs -> Merge -> AudioChunks
        """
        if self.model is None:
            raise RuntimeError("CTC æ¨¡å‹æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥ config ä¸­çš„ chunk_model_path")

        # 1. è®¡ç®—å£°å­¦æ¦‚ç‡ (Forward Pass)
        # audio_tensor: (Time,) -> (1, Time) -> Model
        input_values = self.processor(
            audio_tensor.numpy(), 
            sampling_rate=16000, 
            return_tensors="pt"
        ).input_values.to(self.device)
        
        logits = self.model(input_values).logits  # (1, T, V)
        log_probs = torch.log_softmax(logits, dim=-1).squeeze(0) # (T, V)
        
        # è®¡ç®—æ¯å¸§ç§’æ•° (SPF)
        # num_frames = log_probs.size(0)
        # audio_dur = audio_tensor.size(0) / 16000
        spf = (audio_tensor.size(0) / 16000) / log_probs.size(0)

        # 2. å‘éŸ³å˜ä½“ Beam Search
        prons_per_word = self._words_to_pronunciations(text_list)
        best_candidate = self._beam_search(log_probs, text_list, prons_per_word)

        # 3. Viterbi å¼ºåˆ¶å¯¹é½ (å¾—åˆ° token çº§åˆ«çš„å¯¹é½ç‚¹)
        target_ids = [self.phone_to_id[p] for p in best_candidate.phones]
        trellis = build_trellis(log_probs, target_ids, self.blank_id)
        points = backtrace(trellis, log_probs, target_ids, self.blank_id)
        
        # 4. è½¬æ¢å› Word Segments (ç‰©ç†æ—¶é—´æˆ³)
        # å…ˆè½¬ä¸º token çº§åˆ«çš„ segment
        token_segs = points_to_segments(points, best_candidate.phones)
        
        # å†ç»„åˆæˆ Word
        word_segs = self._phones_to_word_segments_robust(
            token_segs, text_list, 
            [prons_per_word[i][idx] for i, idx in enumerate(best_candidate.pron_choice_idxs)]
        )

        # 5. æ™ºèƒ½åˆå¹¶æˆ Chunk (å®è§‚åˆ‡åˆ†)
        # å°† Segment è½¬æ¢ä¸ºå¸¦çœŸå®æ—¶é—´çš„ WordSeg å¯¹è±¡
        word_objects = [
            WordSeg(s.start_frame * spf, (s.end_frame - s.start_frame) * spf, s.label)
            for s in word_segs
        ]
        
        internal_chunks = self._merge_words_into_chunks(word_objects)
        
        # 6. Padding (å‘ä¸¤ä¾§é™éŸ³å»¶å±•)
        audio_dur_s = audio_tensor.size(0) / 16000
        internal_chunks = self._pad_chunks(internal_chunks, word_objects, audio_dur_s)

        # 7. ç”Ÿæˆæœ€ç»ˆ AudioChunk å¯¹è±¡ (In-Memory Slicing!)
        final_chunks = []
        sr = 16000
        for i, c in enumerate(internal_chunks):
            # è®¡ç®— sample ç´¢å¼•
            s_samp = int(c.start * sr)
            e_samp = int(c.end * sr)
            
            # è¾¹ç•Œä¿æŠ¤
            s_samp = max(0, s_samp)
            e_samp = min(audio_tensor.size(0), e_samp)
            
            if e_samp <= s_samp:
                continue

            # åˆ‡ç‰‡ï¼(Copy to avoid memory leaks if large tensor is kept)
            chunk_tensor = audio_tensor[s_samp:e_samp].clone()
            
            chunk_obj = AudioChunk(
                tensor=chunk_tensor,
                start_time=c.start,
                end_time=c.end,
                text=" ".join(c.words),
                chunk_id=f"chunk_{i:03d}"
            )
            final_chunks.append(chunk_obj)

        return final_chunks

    # --- å†…éƒ¨æ ¸å¿ƒç®—æ³• ---

    def _read_lexicon(self, path: Path):
        lexicon = {}
        if not path.exists():
            return lexicon
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2:
                    word = parts[0]
                    # å…³é”®ï¼šç›´æ¥å­˜ List[str]ï¼Œä¸è¦å¥—å¨ƒ
                    phones = parts[1:] 
                    lexicon[word] = phones
        return lexicon


    def _words_to_pronunciations(self, words: List[str]) -> List[List[List[str]]]:
        out = []
        print(f"\n[DEBUG] --- Entering _words_to_pronunciations (Total words: {len(words)}) ---")
        
        for i, w in enumerate(words):
            w_norm = w.strip().lower()
            if not w_norm:
                continue
                
            if w_norm not in self.lexicon:
                raise ValueError(f"[CTCChunker] OOV Word: {w_norm}")
            
            # 1. åŸå§‹æŸ¥è¡¨ç»“æœ
            flat_phones = self.lexicon[w_norm]
            
            # 2. æ„é€ å½“å‰çš„è¯å€™é€‰ (åŒ…è£¹ä¸€å±‚)
            word_candidates = [flat_phones]
            
            # ğŸ”´ å…³é”®è°ƒè¯•æ‰“å°ï¼šåªæ‰“å°å‰ 3 ä¸ªè¯ï¼Œé˜²æ­¢æ—¥å¿—çˆ†ç‚¸
            if i < 3:
                print(f"[DEBUG] Word[{i}]: '{w_norm}'")
                print(f"        Lexicon says: {flat_phones} (Type: {type(flat_phones)})")
                print(f"        Wrapped into: {word_candidates} (Depth: 2)")
            
            out.append(word_candidates)
        
        # 3. æœ€ç»ˆæ•´ä½“ç»“æ„éªŒè¯
        if out:
            print(f"[DEBUG] Final structure sample (out[0]): {out[0]}")
            # ç‰©ç†éªŒè¯ï¼šå¦‚æœæ˜¯æ­£ç¡®çš„ï¼Œout[0][0] åº”è¯¥æ˜¯ä¸€ä¸ª list (éŸ³ç´ åˆ—è¡¨)ï¼Œè€Œä¸æ˜¯ string
            if len(out[0]) > 0:
                print(f"        Verification: out[0][0][0] is '{out[0][0][0]}' (Should be first phone char)")
        
        print(f"[DEBUG] --- End of _words_to_pronunciations ---\n")
        return out
            


    def _beam_search(self, log_probs, words, prons_per_word) -> PronCandidate:
        """ç®€å•çš„ Beam Searchï¼Œå¯»æ‰¾æœ€ä½³å‘éŸ³ç»„åˆ"""
        # 1. ç¡®ä¿åˆå§‹åŒ–æ—¶ phones æ˜¯ä¸€ä¸ªçº¯å‡€çš„ç©ºåˆ—è¡¨
        beam = [PronCandidate(phones=[], pron_choice_idxs=[], score=0.0)]
        
        for i, word in enumerate(words):
            new_beam = []
            variants = prons_per_word[i] # ç»“æ„: [['t', 'a', ...]]
            
            for cand in beam:
                for p_idx, pron in enumerate(variants):
                    # --- ç»´åº¦é˜²å¾¡æ£€æŸ¥ ---
                    # ç¡®ä¿ pron æ˜¯ ['t', 'a'] è€Œä¸æ˜¯ [['t', 'a']]
                    if len(pron) > 0 and isinstance(pron[0], list):
                        print(f"[DEBUG] Dimension Error detected at word '{word}', flattening...")
                        pron = pron[0] 

                    # 2. æ‹¼æ¥éŸ³ç´ åºåˆ—
                    new_phones = cand.phones + pron
                    
                    # --- å†æ¬¡é˜²å¾¡ï¼šç¡®ä¿ new_phones é‡Œçš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯å­—ç¬¦ä¸² ---
                    try:
                        # æ ¸å¿ƒæŸ¥è¡¨
                        new_ids = [self.phone_to_id[p] for p in new_phones if isinstance(p, str)]
                    except KeyError as e:
                        # æŸäº›éŸ³ç´ ä¸åœ¨è¯è¡¨ä¸­ï¼Œæ‰“å°å‡ºæ¥çœ‹çœ‹æ˜¯å“ªä¸ª
                        # print(f"[CTCChunker] Missing Phone in Vocabulary: {e}")
                        continue
                    except TypeError as e:
                        # å¦‚æœèµ°åˆ°è¿™é‡Œï¼Œè¯´æ˜ new_phones é‡Œæ··è¿›äº† list
                        # æˆ‘ä»¬æ‰“å°å‡º new_phones çš„å‰å‡ ä¸ªå…ƒç´ æ¥æŠ“ç°è¡Œ
                        print(f"[CRITICAL] new_phones sample: {new_phones[:3]}")
                        raise e
                    
                    # 3. è®¡ç®— Viterbi å¾—åˆ† (build_trellis)
                    # è¿™é‡Œçš„ score è®¡ç®—é€»è¾‘ä¿æŒä½ çš„ä¸å˜
                    try:
                        trellis = build_trellis(log_probs, new_ids, self.blank_id)
                        # æ³¨æ„ï¼štrellis ç»´åº¦é€šå¸¸æ˜¯ (T, S+1)
                        score = float(torch.max(trellis[:, len(new_ids)]).item())
                    except Exception:
                        score = -float("inf")
                    
                    new_beam.append(PronCandidate(
                        phones=new_phones,
                        pron_choice_idxs=cand.pron_choice_idxs + [p_idx],
                        score=score
                    ))
            
            # 4. å‰ªæ
            new_beam.sort(key=lambda x: x.score, reverse=True)
            beam = new_beam[:self.beam_size]
            
        return beam[0] if beam else None

    def _phones_to_word_segments_robust(self, token_segs, words, prons):
        """æ ¹æ®éŸ³ç´ é•¿åº¦åæ¨å•è¯è¾¹ç•Œ"""
        word_segs = []
        wi = 0 # token index
        
        for word, pron in zip(words, prons):
            n_phones = len(pron)
            if n_phones == 0:
                continue
            
            if wi + n_phones > len(token_segs):
                break
                
            start_frame = token_segs[wi].start_frame
            end_frame = token_segs[wi + n_phones - 1].end_frame
            
            word_segs.append(Segment(word, start_frame, end_frame))
            wi += n_phones
            
        return word_segs

    def _merge_words_into_chunks(self, words: List[WordSeg]) -> List[InternalChunk]:
        """[æ ¸å¿ƒ] ä½ çš„å­¤å²›åˆ‡åˆ†é€»è¾‘"""
        if not words:
            return []
        
        chunks = []
        cur_words = [words[0].word]
        cur_start = words[0].start
        cur_end = words[0].end
        
        for w in words[1:]:
            gap = w.start - cur_end
            proposed_dur = w.end - cur_start
            
            # åˆ¤æ–­æ˜¯å¦æ–­å¼€
            if gap <= self.max_gap_s and proposed_dur <= self.max_chunk_s:
                # è¿èµ·æ¥
                cur_end = w.end
                cur_words.append(w.word)
            else:
                # æ–­å¼€ï¼Œä¿å­˜å‰ä¸€ä¸ª
                if (cur_end - cur_start) >= self.min_chunk_s and len(cur_words) >= self.min_words:
                    chunks.append(InternalChunk(cur_start, cur_end, cur_words))
                
                # å¼€å¯æ–° Chunk
                cur_start = w.start
                cur_end = w.end
                cur_words = [w.word]
                
        # æ‰«å°¾
        if (cur_end - cur_start) >= self.min_chunk_s and len(cur_words) >= self.min_words:
            chunks.append(InternalChunk(cur_start, cur_end, cur_words))
            
        return chunks

    def _pad_chunks(self, chunks, words, audio_dur):
        """å‘ä¸¤ä¾§å®‰å…¨åœ°å¡«å……é™éŸ³"""
        if not chunks: 
            return []
        out = []
        
        # ä¸ºäº†å¿«é€ŸæŸ¥æ‰¾ï¼Œå»ºç«‹ word map
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šå‡è®¾ chunks å’Œ words éƒ½æ˜¯æœ‰åºçš„
        # å®é™…é€»è¾‘ï¼šåœ¨ words åˆ—è¡¨ä¸­æ‰¾åˆ° chunk çš„ç¬¬ä¸€ä¸ªè¯å’Œæœ€åä¸€ä¸ªè¯çš„å‰åé‚»å±…
        
        # ç®€å•ç‰ˆï¼šåªåšåŸºç¡€ paddingï¼Œä¸å¤„ç†å¤æ‚çš„ word é‚»å±…é¿è®©ï¼ˆä¸ºäº†ä»£ç æ¸…æ™°ï¼‰
        # å¦‚æœéœ€è¦åŸæ¥çš„ä¸¥æ ¼é€»è¾‘ï¼Œå¯ä»¥æŠŠ chunks2.py é‡Œçš„ pad_chunks_without_cutting_words å®Œæ•´æ¬è¿‡æ¥
        # è¿™é‡Œæ¼”ç¤ºåŸºç¡€é€»è¾‘ï¼š
        for c in chunks:
            new_start = max(0.0, c.start - self.pad_s)
            new_end = min(audio_dur, c.end + self.pad_s)
            out.append(InternalChunk(new_start, new_end, c.words))
            
        return out


# ==========================================
#  Static Pure Functions (æ•°å­¦è¿ç®—éƒ¨åˆ†)
# ==========================================

def build_trellis(log_probs: torch.Tensor, targets: List[int], blank_id: int) -> torch.Tensor:
    """Viterbi Trellis æ„å»º (Vectorized)"""
    T, V = log_probs.shape
    N = len(targets)
    device = log_probs.device
    neg_inf = -float("inf")

    targets_t = torch.tensor(targets, device=device, dtype=torch.long)
    trellis = torch.full((T + 1, N + 1), neg_inf, device=device, dtype=log_probs.dtype)
    trellis[0, 0] = 0.0

    # First column: blanks only
    trellis[1:, 0] = torch.cumsum(log_probs[:, blank_id], dim=0)

    for t in range(1, T + 1):
        lp_t = log_probs[t - 1]
        blank = lp_t[blank_id]
        emit_scores = lp_t[targets_t]

        stay = trellis[t - 1, 1:] + blank
        emit = trellis[t - 1, :-1] + emit_scores

        trellis[t, 1:] = torch.maximum(stay, emit)

    return trellis

def backtrace(trellis: torch.Tensor, log_probs: torch.Tensor, targets: List[int], blank_id: int) -> List[Point]:
    """å›æº¯å¯»æ‰¾æœ€ä½³è·¯å¾„ç‚¹"""
    _T = trellis.size(0) - 1
    N = trellis.size(1) - 1
    j = N
    t = int(torch.argmax(trellis[:, j]).item())
    path = []

    while t > 0 and j > 0:
        lp_t = log_probs[t - 1]
        score_stay = trellis[t - 1, j] + lp_t[blank_id]
        score_emit = trellis[t - 1, j - 1] + lp_t[targets[j - 1]]

        if score_emit > score_stay:
            path.append(Point(token_index=j - 1, time_index=t - 1))
            j -= 1
            t -= 1
        else:
            t -= 1
    path.reverse()
    return path

def points_to_segments(points: List[Point], labels: List[str]) -> List[Segment]:
    """å°†ç‚¹è½¬æ¢ä¸ºåŒºé—´"""
    if not points: 
        return []
    segs = []
    for i, p in enumerate(points):
        start = p.time_index
        end = points[i + 1].time_index if i + 1 < len(points) else (p.time_index + 1)
        segs.append(Segment(labels[p.token_index], start, end))
    return segs