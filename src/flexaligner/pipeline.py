import torch
import numpy as np
import time
import pandas as pd
import soundfile as sf
import shutil
import gc
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import Dict, Union, List, Tuple, Optional

# è¿›åº¦æ¡é€‚é…
try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable, **kwargs): return iterable

# [æ ¸å¿ƒä¾èµ–]
from .config import AlignmentConfig
from .frontend import TextFrontend
from .chunker import CTCChunker
from .aligner import LocalAligner

# å®šä¹‰ä¸€ä¸ªè½»é‡çº§çš„æ•°æ®ç»“æ„ï¼Œç”¨äºåœ¨ Stage 1 å’Œ Stage 2 ä¹‹é—´ä¼ é€’æ•°æ®
@dataclass
class AlignmentTask:
    chunk_id: str
    text: str
    start_time: float
    end_time: float
    tensor: torch.Tensor

class FlexAligner:
    def __init__(self, config: Union[Dict, AlignmentConfig, None] = None):
        """
        FlexAligner æ§åˆ¶å™¨ï¼šä¸¥æ ¼åˆ†æ­¥æ‰§è¡Œ (Segmentation -> Alignment)
        """
        # 1. é…ç½®åŠ è½½
        if isinstance(config, dict):
            self.config = AlignmentConfig(**config)
        elif isinstance(config, AlignmentConfig):
            self.config = config
        else:
            self.config = AlignmentConfig()
        
        self.config_dict = asdict(self.config)
        
        # 2. åˆå§‹åŒ–å‰ç«¯ (è½»é‡çº§ï¼Œå¸¸é©»)
        mode = getattr(self.config, "validation_mode", "FAST")
        self.frontend = TextFrontend(config=self.config_dict, mode=mode)
        
        # 3. æ¨¡å‹ç»„ä»¶ (æ‡’åŠ è½½ / æŒ‰éœ€åŠ è½½ï¼Œåˆå§‹ä¸ºç©º)
        self.chunker: Optional[CTCChunker] = None
        self.aligner: Optional[LocalAligner] = None

    # =========================================================================
    # [å…¥å£ 1] å•æ–‡ä»¶å¤„ç† (Strict Mode)
    # =========================================================================
    def align(self, audio_path: str, text_path: str, output_path: str, verbose: bool = True):
        """
        å•æ–‡ä»¶å…¨æµç¨‹ï¼šé’ˆå¯¹å•æ–‡ä»¶ä»»åŠ¡ï¼Œå¦‚æœå‡ºé”™ï¼Œåº”å½“ç›´æ¥æŠ›å‡ºå¼‚å¸¸ã€‚
        """
        tasks = [(audio_path, text_path, output_path)]
        self.align_batch(tasks, raise_on_error=True)

    # =========================================================================
    # [å…¥å£ 2] æ‰¹é‡å¤„ç† (Robust Mode - Two Stage)
    # =========================================================================
    def align_batch(self, tasks: List[Tuple[str, str, str]], raise_on_error: bool = False):
        """
        ä¸¥æ ¼åˆ†æ­¥æ‰§è¡Œæ‰¹é‡ä»»åŠ¡ï¼š
        Phase 1: å…¨éƒ¨åˆ‡åˆ† -> å†…å­˜æš‚å­˜ -> å¸è½½ Chunker
        Phase 2: åŠ è½½ Aligner -> è¯»å–æš‚å­˜ -> å¯¹é½æ‹¼æ¥
        """
        if not tasks: return

        print("\n" + "="*80)
        print(f"ğŸš€ [FlexAligner] Batch Processing: {len(tasks)} files")
        print(f"   Strategy: Two-Stage Sequential (Chunk -> Align)")
        print("="*80)

        # --- Phase 1: Segmentation ---
        print(f"\n[Phase 1] Segmentation & Text Preprocessing...")
        
        if self.chunker is None:
            print(f"   -> Loading Chunker model...")
            self.chunker = CTCChunker(config=self.config_dict)
        
        # æš‚å­˜æ‰€æœ‰æ–‡ä»¶çš„ Chunk ä¿¡æ¯
        # ç»“æ„: [ {"output_path": str, "full_duration": float, "chunks": List[AlignmentTask]}, ... ]
        batch_data = []
        
        pbar_seg = tqdm(tasks, desc="Seg", unit="file")
        for audio_p, text_p, out_p in pbar_seg:
            try:
                # IO
                audio_np = self.frontend.load_audio(audio_p)
                raw_text = self.frontend.load_text(text_p)
                
                # Preprocess
                lang = self.config.lang if self.config.lang else self.frontend.detect_language(raw_text)
                tokens = self.frontend.get_phonemes(raw_text, lang)
                text_list = [t.strip() for t in tokens if t.strip()]
                
                audio_tensor = torch.from_numpy(audio_np).float()
                full_dur = audio_tensor.size(0) / 16000.0
                
                # Chunking
                file_id = Path(audio_p).stem
                raw_chunks = self.chunker.find_chunks(audio_tensor, text_list, file_id=file_id)
                
                # è½¬æ¢ä¸ºæ ‡å‡† Task å¯¹è±¡
                task_chunks = []
                for rc in raw_chunks:
                    task_chunks.append(AlignmentTask(
                        chunk_id=rc.chunk_id,
                        text=rc.text,
                        start_time=rc.start_time,
                        end_time=rc.end_time,
                        tensor=rc.tensor
                    ))

                batch_data.append({
                    "output_path": out_p,
                    "full_duration": full_dur,
                    "chunks": task_chunks,
                    "src_name": Path(audio_p).name
                })
                
            except Exception as e:
                if raise_on_error: raise e
                tqdm.write(f"âŒ Segmentation Failed {Path(audio_p).name}: {e}")
        
        # æ˜¾å­˜æ¸…ç†
        print(f"   -> Unloading Chunker to free VRAM...")
        del self.chunker
        self.chunker = None
        gc.collect()
        if torch.cuda.is_available(): torch.cuda.empty_cache()

        # --- Phase 2: Alignment ---
        print(f"\n[Phase 2] Alignment & Stitching...")
        if not batch_data: return

        if self.aligner is None:
            print(f"   -> Loading Aligner model...")
            self.aligner = LocalAligner(config=self.config_dict)

        pbar_ali = tqdm(batch_data, desc="Align", unit="file")
        for item in pbar_ali:
            try:
                # è°ƒç”¨ç»Ÿä¸€çš„ç¼åˆé€»è¾‘
                self._stitch_and_export(
                    chunks=item['chunks'],
                    full_duration=item['full_duration'],
                    output_path=item['output_path']
                )
            except Exception as e:
                if raise_on_error: raise e
                tqdm.write(f"âŒ Alignment Failed {item['src_name']}: {e}")

        print("\n" + "="*80)
        print(f"ğŸ Batch Processing Completed.")
        print("="*80 + "\n")

    # =========================================================================
    # [å…¥å£ 3] ä» Manifest æ¢å¤ (Stage 2 Only)
    # =========================================================================
    def align_from_manifest(
        self, 
        manifest_path: str, 
        audio_dir: str, 
        output_path: str, 
        full_audio_path: Optional[str] = None,
        verbose: bool = True
    ):
        """
        Stage 2 ç‹¬ç«‹æ¨¡å¼ï¼šè¯»å– TSV -> å¯»æ‰¾éŸ³é¢‘ -> è½¬æ¢ä¸º Task å¯¹è±¡ -> ç»Ÿä¸€ç¼åˆ
        """
        tsv_path = Path(manifest_path)
        wav_dir_path = Path(audio_dir)
        
        if not tsv_path.exists(): raise FileNotFoundError(f"Manifest not found: {tsv_path}")
        if not wav_dir_path.exists(): raise FileNotFoundError(f"Chunk audio dir: {wav_dir_path}")

        if verbose:
            print(f"ğŸ§© [Resume] Processing {tsv_path.name}")

        if self.aligner is None:
            self.aligner = LocalAligner(config=self.config_dict)

        # 1. ç¡®å®šæ€»æ—¶é•¿
        target_duration = 0.0
        if full_audio_path and Path(full_audio_path).exists():
            target_duration = sf.info(full_audio_path).duration
        
        # 2. è¯»å– TSV
        try:
            df = pd.read_csv(tsv_path, sep='\t')
        except Exception as e:
            raise RuntimeError(f"Failed to parse TSV: {e}")

        if target_duration == 0.0 and not df.empty:
            # é™çº§ï¼šä¼°ç®—
            try: target_duration = float(df.iloc[-1]['end_s'])
            except: target_duration = 0.0

        # 3. æ„å»º Task åˆ—è¡¨ (æ¨¡æ‹Ÿ Phase 1 çš„è¾“å‡º)
        task_chunks = []
        for _, row in tqdm(df.iterrows(), total=len(df), desc="Loading Audio", disable=not verbose):
            chunk_id = row['chunk_id']
            text = str(row.get('text', row.get('words', ''))).strip()
            start = float(row['start_s'])
            end = float(row['end_s'])

            # å¯»æ‰¾éŸ³é¢‘ (å…¼å®¹ Legacy å‘½å {id}_*.wav)
            candidates = list(wav_dir_path.glob(f"{chunk_id}_*.wav"))
            if not candidates:
                candidates = list(wav_dir_path.glob(f"{chunk_id}.wav"))
            
            if not candidates:
                if verbose: print(f"âŒ Chunk audio missing: {chunk_id}")
                continue
            
            # è¯»å–éŸ³é¢‘è½¬ Tensor
            try:
                wav, sr = sf.read(str(candidates[0]))
                if sr != 16000 and verbose: print(f"âš ï¸ Resampling required for {chunk_id}")
                chunk_tensor = torch.from_numpy(wav).float()
                if chunk_tensor.ndim > 1: chunk_tensor = chunk_tensor.mean(dim=1)
                
                task_chunks.append(AlignmentTask(
                    chunk_id=chunk_id,
                    text=text,
                    start_time=start,
                    end_time=end,
                    tensor=chunk_tensor
                ))
            except Exception as e:
                print(f"âŒ Error loading {chunk_id}: {e}")

        # 4. è°ƒç”¨ç»Ÿä¸€ç¼åˆé€»è¾‘
        self._stitch_and_export(task_chunks, target_duration, output_path)
        if verbose: print(f"âœ… Saved to {output_path}")

    # =========================================================================
    # [æ ¸å¿ƒç§æœ‰æ–¹æ³•] ç¼åˆä¸å¯¼å‡º (The Core Stitcher)
    # =========================================================================
    def _stitch_and_export(self, chunks: List[AlignmentTask], full_duration: float, output_path: str):
        """
        æ ¸å¿ƒé€»è¾‘ï¼šæ¥æ”¶æ ‡å‡†åŒ– Chunk åˆ—è¡¨ï¼Œæ‰§è¡Œå¯¹é½ï¼Œå¤„ç† Gap/Paddingï¼Œç”Ÿæˆ TextGrid
        """
        global_phones = []
        global_words = []
        prev_global_end = 0.0  # ç‰©ç†é”šç‚¹å½’é›¶

        for chunk in chunks:
            chunk_start = chunk.start_time
            chunk_end = chunk.end_time
            
            # A. æ ¸å¿ƒå¯¹é½æ¨ç†
            result = self.aligner.align_locally(chunk.tensor, chunk.text, file_id=chunk.chunk_id)
            
            if not result['phones']: 
                # å¦‚æœå¯¹é½å¤±è´¥ï¼Œä¸ºäº†ä¿æŒæ—¶é—´è½´è¿ç»­ï¼Œå¯èƒ½éœ€è¦å¡«è¡¥ï¼Ÿ
                # ç›®å‰é€»è¾‘æ˜¯è·³è¿‡ï¼Œè¿™ä¼šå¯¼è‡´å¤§ Gap
                continue

            # B. å¤´éƒ¨ç¼åˆ (Stitch Gap)
            gap = chunk_start - prev_global_end
            if gap > 0.001:
                gap_seg = ("NULL", prev_global_end, chunk_start)
                global_phones.append(gap_seg)
                global_words.append(gap_seg)
            
            # C. æ·»åŠ å¯¹é½ç»“æœ (Offset Shift)
            for seg in result['phones']:
                global_phones.append((seg.label, chunk_start + seg.start, chunk_start + seg.end))
            for seg in result['words']:
                global_words.append((seg.label, chunk_start + seg.start, chunk_start + seg.end))
            
            prev_global_end = chunk_end

        # D. å°¾éƒ¨è¡¥é½ (Final Padding)
        # è·å–æœ€åä¸€ä¸ªæœ‰æ•ˆå¯¹é½ç‚¹çš„ç»“æŸæ—¶é—´
        final_valid_end = max(prev_global_end, global_phones[-1][2] if global_phones else 0.0)
        
        if full_duration > final_valid_end + 0.001:
            pad_seg = ("NULL", final_valid_end, full_duration)
            global_phones.append(pad_seg)
            global_words.append(pad_seg)
            final_valid_end = full_duration # æ›´æ–°ä¸ºçœŸå®æ—¶é•¿

        # E. å†™å…¥æ–‡ä»¶
        self._export_textgrid_file(output_path, final_valid_end, {"phones": global_phones, "words": global_words})

    def _export_textgrid_file(self, path: str, duration: float, tiers_data: dict):
        """åº•å±‚ I/Oï¼šTextGrid æ ¼å¼åŒ–å†™å…¥"""
        p = Path(path)
        p.parent.mkdir(parents=True, exist_ok=True)
        
        def fmt(val): return f"{val:.6f}"

        def format_tier(name, segments):
            lines = []
            lines.append('        class = "IntervalTier"')
            lines.append(f'        name = "{name}"')
            lines.append('        xmin = 0') 
            lines.append(f'        xmax = {fmt(duration)}') 
            lines.append(f'        intervals: size = {len(segments)}')
            
            for i, (label, start, end) in enumerate(segments):
                lines.append(f'        intervals [{i+1}]:')
                lines.append(f'            xmin = {fmt(start)}')
                lines.append(f'            xmax = {fmt(end)}')
                safe_label = str(label).replace('"', '""')
                lines.append(f'            text = "{safe_label}"')
            return lines

        lines = [
            'File type = "ooTextFile"',
            'Object class = "TextGrid"',
            '',
            'xmin = 0',
            f'xmax = {fmt(duration)}',
            'tiers? <exists>',
            f'size = {len(tiers_data)}',
            'item []:'
        ]
        
        tier_idx = 1
        for name in ["words", "phones"]:
            if name in tiers_data:
                lines.append(f'    item [{tier_idx}]:')
                lines.extend(format_tier(name, tiers_data[name]))
                tier_idx += 1
                
        content = "\n".join(lines) + "\n"
        p.write_text(content, encoding="utf-8")